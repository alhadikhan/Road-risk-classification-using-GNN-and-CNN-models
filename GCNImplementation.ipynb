{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b94355f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iftii\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\iftii\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\iftii\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\iftii\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\iftii\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\iftii\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "271/271 [==============================] - 180s 650ms/step - loss: 0.6016 - accuracy: 0.8582 - val_loss: 0.1094 - val_accuracy: 0.9584\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - 180s 666ms/step - loss: 0.1065 - accuracy: 0.9687 - val_loss: 0.0344 - val_accuracy: 0.9843\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - 176s 649ms/step - loss: 0.0514 - accuracy: 0.9827 - val_loss: 0.0075 - val_accuracy: 0.9954\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - 177s 651ms/step - loss: 0.0191 - accuracy: 0.9946 - val_loss: 0.0091 - val_accuracy: 0.9972\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - 179s 662ms/step - loss: 0.0511 - accuracy: 0.9869 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - 175s 646ms/step - loss: 0.0141 - accuracy: 0.9956 - val_loss: 2.3693e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - 180s 664ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 3.5102e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - 173s 640ms/step - loss: 0.0747 - accuracy: 0.9781 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - 177s 655ms/step - loss: 0.0341 - accuracy: 0.9903 - val_loss: 4.5461e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - 177s 652ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 8.8626e-04 - val_accuracy: 0.9991\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Image 13.jpg: Predicted class - 0\n",
      "Image 292.jpg: Predicted class - 1\n",
      "Image 44.jpg: Predicted class - 2\n",
      "Image 500.jpg: Predicted class - 0\n",
      "Image 519.jpg: Predicted class - 1\n",
      "Image 521.jpg: Predicted class - 2\n",
      "Image 600.jpg: Predicted class - 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(data_folder):\n",
    "    X, y = [], []\n",
    "    categories = [\"class_0\", \"class_1\", \"class_2\"]\n",
    "    for category_id, category in enumerate(categories):\n",
    "        category_folder = os.path.join(data_folder, category)\n",
    "        num_images = [4000, 4000, 3000][category_id]  # Adjusting for the corrected count\n",
    "        for i in range(1, num_images + 1):  # Start from 1 instead of 0\n",
    "            img_path = os.path.join(category_folder, f\"{i}.jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (224, 224))  # Resize images to the same size\n",
    "                    X.append(img)\n",
    "                    y.append(category_id)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Load the dataset\n",
    "data_folder = \"dataset/train\"\n",
    "X, y = load_dataset(data_folder)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define GCN model for image classification\n",
    "def create_gcn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (224, 224, 3)  # Input shape depends on your image size\n",
    "num_classes = 3  # Number of classes in your dataset\n",
    "\n",
    "# Create GCN model\n",
    "gcn_model = create_gcn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "gcn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "gcn_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = gcn_model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Predict on new images\n",
    "def predict_on_images(model, image_folder):\n",
    "    predictions = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (224, 224))  # Resize images to the same size as the training images\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "            pred = model.predict(img)\n",
    "            predicted_class = np.argmax(pred)\n",
    "            predictions.append((filename, predicted_class))\n",
    "    return predictions\n",
    "\n",
    "# Test on new images\n",
    "new_images_folder = \"dataset/test\"  # Updated folder name\n",
    "predictions = predict_on_images(gcn_model, new_images_folder)\n",
    "for filename, predicted_class in predictions:\n",
    "    print(f\"Image {filename}: Predicted class - {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c989d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
